<!DOCTYPE html>
<!-- saved from url=(0075)http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06 -->
<html lang="en" dir="ltr" class="js desktop gr__students_iitk_ac_in"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>2014pc:view_projects06 [project wiki]</title>
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>
    <meta name="generator" content="DokuWiki">
<meta name="robots" content="index,follow">
<meta name="date" content="2014-06-24T15:49:06+0530">
<meta name="keywords" content="2014pc,view_projects06">
<link rel="search" type="application/opensearchdescription+xml" href="http://students.iitk.ac.in/projects/wiki/lib/exe/opensearch.php" title="project wiki">
<link rel="start" href="http://students.iitk.ac.in/projects/wiki/">
<link rel="contents" href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=index" title="Sitemap">
<link rel="alternate" type="application/rss+xml" title="Recent changes" href="http://students.iitk.ac.in/projects/wiki/feed.php">
<link rel="alternate" type="application/rss+xml" title="Current namespace" href="http://students.iitk.ac.in/projects/wiki/feed.php?mode=list&ns=2014pc">
<link rel="alternate" type="text/html" title="Plain HTML" href="http://students.iitk.ac.in/projects/wiki/doku.php?do=export_xhtml&id=2014pc:view_projects06">
<link rel="alternate" type="text/plain" title="Wiki Markup" href="http://students.iitk.ac.in/projects/wiki/doku.php?do=export_raw&id=2014pc:view_projects06">
<link rel="canonical" href="./emodet_wiki_files/emodet_wiki.html">
<link rel="stylesheet" type="text/css" href="./emodet_wiki_files/css.php">
<script type="text/javascript">/*<![CDATA[*/var NS='2014pc';var JSINFO = {"id":"2014pc:view_projects06","namespace":"2014pc"};
/*!]]>*/</script>
<script type="text/javascript" charset="utf-8" src="./emodet_wiki_files/js.php"></script>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <link rel="shortcut icon" href="http://students.iitk.ac.in/projects/wiki/lib/tpl/dokuwiki/images/favicon.ico">
<link rel="apple-touch-icon" href="http://students.iitk.ac.in/projects/wiki/lib/tpl/dokuwiki/images/apple-touch-icon.png">
    </head>

<body>
    <!--[if lte IE 7 ]><div id="IE7"><![endif]--><!--[if IE 8 ]><div id="IE8"><![endif]-->
    <div id="dokuwiki__site"><div id="dokuwiki__top" class="site dokuwiki mode_show tpl_dokuwiki     ">

        
<!-- ********** HEADER ********** -->
<div id="dokuwiki__header"><div class="pad group">

    
    <div class="headings group">
        <ul class="a11y skip">
            <li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06#dokuwiki__content">skip to content</a></li>
        </ul>

        <h1><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=start" accesskey="h" title="[H]"><img src="./emodet_wiki_files/fetch.php" width="64" height="64" alt=""> <span>project wiki</span></a></h1>
            </div>

    <div class="tools group">
        <!-- USER TOOLS -->
                    <div id="dokuwiki__usertools">
                <h3 class="a11y">User Tools</h3>
                <ul>
                    <li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=login&sectok=8a3679089309d79ca2ccbefd81076e9b" class="action login" rel="nofollow" title="Login">Login</a></li>                </ul>
            </div>
        
        <!-- SITE TOOLS -->
        <div id="dokuwiki__sitetools">
            <h3 class="a11y">Site Tools</h3>
            <form action="http://students.iitk.ac.in/projects/wiki/doku.php?id=start" accept-charset="utf-8" class="search" id="dw__search" method="get" role="search"><div class="no"><input type="hidden" name="do" value="search"><input type="text" id="qsearch__in" accesskey="f" name="id" class="edit" title="[F]"><input type="submit" value="Search" class="button" title="Search"><div id="qsearch__out" class="ajax_qsearch JSpopup" style="display: none;"></div></div></form>            <div class="mobileTools">
                <form action="http://students.iitk.ac.in/projects/wiki/doku.php" method="get" accept-charset="utf-8"><div class="no"><input type="hidden" name="id" value="2014pc:view_projects06"><select name="do" class="edit quickselect" title="Tools"><option value="">Tools</option><optgroup label="Page Tools"><option value="edit">Show pagesource</option><option value="revisions">Old revisions</option><option value="backlink">Backlinks</option></optgroup><optgroup label="Site Tools"><option value="recent">Recent changes</option><option value="media">Media Manager</option><option value="index">Sitemap</option></optgroup><optgroup label="User Tools"><option value="login">Login</option></optgroup></select><input type="submit" value="&gt;" style="display: none;"></div></form>            </div>
            <ul>
                <li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=recent" class="action recent" accesskey="r" rel="nofollow" title="Recent changes [R]">Recent changes</a></li><li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=media&ns=2014pc" class="action media" rel="nofollow" title="Media Manager">Media Manager</a></li><li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=index" class="action index" accesskey="x" rel="nofollow" title="Sitemap [X]">Sitemap</a></li>            </ul>
        </div>

    </div>

    <!-- BREADCRUMBS -->
            <div class="breadcrumbs">
                                        <div class="trace"><span class="bchead">Trace:</span> <span class="bcsep">•</span> <span class="curid"><bdi><a href="./emodet_wiki_files/emodet_wiki.html" class="breadcrumbs" title="2014pc:view_projects06">view_projects06</a></bdi></span></div>
                    </div>
    
    
    <hr class="a11y">
</div></div><!-- /header -->

        <div class="wrapper group">

            
            <!-- ********** CONTENT ********** -->
            <div id="dokuwiki__content"><div class="pad group">

                <div class="pageId"><span>2014pc:view_projects06</span></div>

                <div class="page group">
                                                            <!-- wikipage start -->
                    <!-- TOC START -->
<div id="dw__toc">
<h3 class="toggle open" style="cursor: pointer;"><strong><span>−</span></strong>Table of Contents</h3>
<div>

<ul class="toc" aria-expanded="true" style="">
<li class="level1"><div class="li"><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06#affective_computing">Affective Computing</a></div>
<ul class="toc">
<li class="level2"><div class="li"><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06#project_members">Project Members:</a></div></li>
<li class="level2"><div class="li"><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06#project_info">Project Info:</a></div></li>
<li class="level2"><div class="li"><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06#project_log">Project Log:</a></div></li>
</ul></li>
</ul>
</div>
</div>
<!-- TOC END -->

<h1 class="sectionedit1" id="affective_computing">Affective Computing</h1>
<div class="level1">

<p>
<br>

</p>

</div>

<h2 class="sectionedit2" id="project_members">Project Members:</h2>
<div class="level2">

<p>
<br>

 - Nishant Rai<br>

 - Atanu Chakraborty<br>

 - Sahil Grover <br>

 - Amlan Kar<br>

</p>
<hr>

</div>

<h2 class="sectionedit3" id="project_info">Project Info:</h2>
<div class="level2">

<p>
<br>

<code><strong>Inspiration for our approach :</strong></code><br>
<br>

Our project in simple words is ” Emotion Recognition”. Let's say we have a video, in which we have to decide the emotions of a subject, the general approach towards it is to use only the facial expressions to decide  the emotion. But this has some drawbacks, some emotions are confused with others.<br>

So our approach is  basically using a triad based method i.e. considering all visual, audio and textual data for classification. We expect this to increase the accuracy of the classifier. Let's say the visual-emotion classifier is confused  between some emotions, then the audio-classifier may come to the rescue. So this is the inspiration for our three way approach to emotion recognition.
</p>

<p>
(Visual data can be timed shots of the video, audio data are the features of the voice (of the subject), textual data is similar to the subtitles of the video (can be obtained by a speech-text api)).
</p>

<p>
<code><strong>GitHub Repository :</strong></code><br>
<br>

The link to our repo is : <a href="https://github.com/nishantrai18/affective_computing" class="urlextern" title="https://github.com/nishantrai18/affective_computing" rel="nofollow">Github Repo</a>.<br>
<br>

</p>
<hr>

</div>

<h2 class="sectionedit4" id="project_log">Project Log:</h2>
<div class="level2">

<p>
<br>

</p>

</div>

<h5 id="may">17 May</h5>
<div class="level5">

<p>
 - Text Semantic Analysis is tough ! Need to make a database manually, corpus not available easily, still searching. 
</p>
<hr>

</div>

<h5 id="may1">18 May</h5>
<div class="level5">

<p>
 <code><strong>Interesting Research Papers</strong></code> : <br>

 - We read a couple of research papers, out of which these were the most helpful:<br>

 - Text Semantics Analysis : <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4427100" class="urlextern" title="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4427100" rel="nofollow">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4427100</a><br>

 - A wonderful paper on emotion recognition in sentences : <a href="http://www.scielo.org.mx/pdf/poli/n45/n45a7.pdf" class="urlextern" title="http://www.scielo.org.mx/pdf/poli/n45/n45a7.pdf" rel="nofollow">http://www.scielo.org.mx/pdf/poli/n45/n45a7.pdf</a><br>

 - Recognizing Emotions in Text : <a href="http://www-scf.usc.edu/~saman/pubs/2007-MS-Thesis.pdf" class="urlextern" title="http://www-scf.usc.edu/~saman/pubs/2007-MS-Thesis.pdf" rel="nofollow">http://www-scf.usc.edu/~saman/pubs/2007-MS-Thesis.pdf</a><br>

</p>

<p>
 <code><strong>Corpora found</strong></code> : 
 - We found these useful corpora:<br>

 - A good Image Database : <a href="http://www.kasrl.org/jaffe.html" class="urlextern" title="http://www.kasrl.org/jaffe.html" rel="nofollow">http://www.kasrl.org/jaffe.html</a><br>

 - A good source of corpora : <a href="http://emotion-research.net/wiki/Databases" class="urlextern" title="http://emotion-research.net/wiki/Databases" rel="nofollow">http://emotion-research.net/wiki/Databases</a> <br>

 - Emotional Prosody Speech and Transcripts : <a href="https://catalog.ldc.upenn.edu/LDC2002S28" class="urlextern" title="https://catalog.ldc.upenn.edu/LDC2002S28" rel="nofollow">https://catalog.ldc.upenn.edu/LDC2002S28</a>. But need permission to access it.<br>

 - Mailed the corpus TA at Stanford (Natalia Silveira) for possible access to the Emotional Prosody Speech and Transcripts corpora that's stored on the Stanford servers. <br>

 - A text corpora with relevant data for our emotion classifier : <a href="http://www.cse.unt.edu/~rada/affectivetext/" class="urlextern" title="http://www.cse.unt.edu/~rada/affectivetext/" rel="nofollow">http://www.cse.unt.edu/~rada/affectivetext/</a><br>

 - Not exactly a corpora (sort of a dictionary) : <a href="http://sentiwordnet.isti.cnr.it/" class="urlextern" title="http://sentiwordnet.isti.cnr.it/" rel="nofollow">http://sentiwordnet.isti.cnr.it/</a>.<br>

</p>

<p>
 - We have made some training data for it manually (Courtesy Atanu and Sahil).<br>

 - After some discussion, we decide to assign low priority to the text-emotion classifier. Due to the fact that in normal conversations you really can't decide the emotion solely on the basis of the sentences. We may remake the classifier afterwards.<br>

 - <code><strong>Classifier #0 : </strong></code>So moving on… finally, a basic emotion classifier has been completed. Feels good to get things started. The approach was based on finding which 'words' correspond to which emotion, and then a simple '<a href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" class="urlextern" title="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="nofollow">nearest k</a>' like approach.<br>

 - Thinking of a better second approach for the emotion classifier after reading some of the above mentioned papers.<br>

</p>
<hr>

</div>

<h5 id="may2">19 May</h5>
<div class="level5">

<p>
 - Trying to make a slightly different classifier which decides how much a sentence is positive or negative.<br>

 - <code><strong>Decided to do two implementations</strong></code>:<br>

 - One on the basis of the <a href="http://sentiwordnet.isti.cnr.it/" class="urlextern" title="http://sentiwordnet.isti.cnr.it/" rel="nofollow">SentiWordNet</a> corpus, it has a large list of words with their associated positive and negative scores. We plan &lt;br&gt; to do a simple approach consisting of the general housekeeping (removing stopwords, tokenisation, stemming, etc) and then summing up the scores of the words and averaging it.<br>

 - The second uses <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" class="urlextern" title="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="nofollow">Naïve Bayes</a> approach on following dataset : <a href="http://www.cs.cornell.edu/People/pabo/movie-review-data/" class="urlextern" title="http://www.cs.cornell.edu/People/pabo/movie-review-data/" rel="nofollow">http://www.cs.cornell.edu/People/pabo/movie-review-data/</a>. The feature vector consist of the dominant affective words remaining after the housekeeping.<br>

</p>
<hr>

</div>

<h5 id="may3">20 May</h5>
<div class="level5">

<p>
 - Wrote a code to extract relevant information from the complicated SentiWordNet 3.0 corpus. The output was written in another text file for future use.<br>

 - <code><strong>Classifier #1 : </strong></code>Tried to do an improve our approach by considering<a href="http://en.wikipedia.org/wiki/Part-of-speech_tagging" class="urlextern" title="http://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="nofollow"> POS (part of speech)</a> in a sentence. Handled nouns, adverbs, adjectives and verbs. Modifiers were handled differently, by multiplying their weights with those of the future words. POS tagging requires additional data, but thanks to an already implemented POS-Tagger in NLTK we didn't have to search for it.<br>

 - The following links were referred for <code><strong>POS Tagging</strong></code>:<br>

 - <a href="http://www.monlp.com/2011/11/08/part-of-speech-tags/" class="urlextern" title="http://www.monlp.com/2011/11/08/part-of-speech-tags/" rel="nofollow">http://www.monlp.com/2011/11/08/part-of-speech-tags/</a><br>

 - <a href="http://www.nltk.org/book/ch05.html" class="urlextern" title="http://www.nltk.org/book/ch05.html" rel="nofollow">http://www.nltk.org/book/ch05.html</a><br>

 - We tried to classify many simple sentences, and our code produced nice results for a majority of them. Even complicated articles were classified somewhat correctly.<br>

</p>
<hr>

</div>

<h5 id="may4">21 May</h5>
<div class="level5">

<p>
 - <code><strong>Classifier #2 : </strong></code>Finished the Naive Bayes classifier, it's a general approach consisting of some housekeeping and a standard Bernoulli model. The accuracy of the classifier on the movie review dataset is around 0.7 (which is reasonable). <br>

 - <code><strong>Classifier #3 : </strong></code>Thinking of a slight improvement in the Naïve Bayes classifier, it consist of marking words according to their POS and then using the Naïve Bayes approach. Had to make a program to check it's accuracy.  Ironically, this reduces the accuracy to 0.55, it maybe because the dataset contains only review statements, not natural ones which occur in day to day conversations.<br>

 - <code><strong>Classifier #4 : </strong></code>Thinking of improving classifier #1 by merging it with <a href="http://en.wikipedia.org/wiki/Support_vector_machine" class="urlextern" title="http://en.wikipedia.org/wiki/Support_vector_machine" rel="nofollow">support vector classification</a>. Made a program to check it's accuracy, somehow even this fails to surpass Classifier #2 in terms of accuracy. The accuracy is around 0.6.<br>

 - <code><strong>Classifier #5 : </strong></code>Tried to use Classifier #1 with Support Vector Regression instead. The accuracy changes were very minor.<br>

 - Classifier #1, #4, #5 had an unconventional and original approach, so it feels bad that they didn't work out as expected. But I still doubt the quality of the test-train data. <br>

 - So we have tried almost 6 major implementations (with a lot of hit and trial to find the ideal features). This officially brings an end to the text-classification part (for now).<br>

 - One interesting result is that positive sentiments are classified more accurately, around 0.75 or 0.8 in all of them. While the negatives are classified poorly.<br>

 - Installed Git for Windows on our computers, finished uploading all the classifiers to the github repo. <br>

</p>
<hr>

</div>

<h5 id="may5">22 May</h5>
<div class="level5">

<p>
 - Installed <a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:opencv.org" class="wikilink2" title="2014pc:opencv.org" rel="nofollow">OpenCV</a> in Windows, integrated it with python. Thus all the code can be written in one language.<br>

 - Learnt basic functions such as picture and video I/O, using the laptop's camera in the program, etc.<br>

</p>
<hr>

</div>

<h5 id="may6">23 May</h5>
<div class="level5">

<p>
 - <code><strong>Classifier #6 : </strong></code>Added a modified Naive Bayes classifier to the git repo. This time the accuracy has gone up to 0.75 on the movie review dataset.<br>

</p>

<p>
- Learnt about the <a href="https://docs.python.org/2/library/pickle.html" class="urlextern" title="https://docs.python.org/2/library/pickle.html" rel="nofollow">Pickle</a> Library in python, it will be an integral part of all our classifiers. The reason is that it helps in saving the state of a running program :<br>

- Let's say, I have some data as input in my program and I do loads of operations on it to get a final number. This process takes around 2 hours, now I want to use this 'output' in some other program. I 
 obviously won't do that whole operation all over again in the other program, so what to do, I could definitely write down the value manually, but if it's a huge array then it's gonna create some problems. So
 this is exactly what pickle does. We will personally use it for saving trained classifiers, because we don't want to train it every time we run it.<br>

 - Learnt more OpenCV functions and operations like changing color spaces, image smoothing, blurring, edge detection, etc. Made a program which only shows a specified color (blue and red) in a video input.<br>

</p>
<hr>

</div>

<h5 id="may7">24 May</h5>
<div class="level5">

<p>
 - During the project review, we were told about a good <abbr title="Application Programming Interface">API</abbr> for sentiment analysis : Alchemy, <a href="http://www.alchemyapi.com/products/features/sentiment-analysis/" class="urlextern" title="http://www.alchemyapi.com/products/features/sentiment-analysis/" rel="nofollow">http://www.alchemyapi.com/products/features/sentiment-analysis/</a>.<br>

 - But it will just destroy the 6-8 classifiers we have made till now, we won't be able to learn anything significant about NLP (Natural Language Processing) if we use it, thus defeating the whole purpose of this project. Having said that, it really is a “GREAT” <abbr title="Application Programming Interface">API</abbr>! So we are keeping it on hold, if we aren't able to get good results from our own classifiers then we shall use it.<br>

 - Integrated the Pickle library with the rest of the code.<br>

 - Continued learning more about OpenCV and its functions.
</p>
<hr>

</div>

<h5 id="may8">25 May</h5>
<div class="level5">

<p>
 - Halfway through the OpenCV tutorials, we have learnt and made programs for <a href="http://en.wikipedia.org/wiki/Hough_transform" class="urlextern" title="http://en.wikipedia.org/wiki/Hough_transform" rel="nofollow">Hough transforms</a>, Grabcut algorithm.<br>

- Learning about feature detection (the relevant part for our project).<br>

</p>
<hr>

</div>

<h5 id="may9">26 May</h5>
<div class="level5">

<p>
 - Finished making programs for various feature detection techniques : Corner detection, line detection, etc.<br>

 - Finished the rest of the tutorial. Some interesting concepts which maybe useful are background subtraction, optical flow.<br>

 - Stuck with <a href="https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&sqi=2&ved=0CBsQFjAA&url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHaar-like_features&ei=1-KnU4bSL8miugSqgIHgAg&usg=AFQjCNFXtQsfi3O2bfi78UNI1SGahmNK7g&bvm=bv.69411363,d.c2E" class="urlextern" title="https://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;sqi=2&amp;ved=0CBsQFjAA&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FHaar-like_features&amp;ei=1-KnU4bSL8miugSqgIHgAg&amp;usg=AFQjCNFXtQsfi3O2bfi78UNI1SGahmNK7g&amp;bvm=bv.69411363,d.c2E" rel="nofollow">Haar Cascade</a> function, it isn't giving any result. Searching the web for possible solutions.<br>

 - Finally finished the Haar Cascade program, somewhat a silly mistake. It finds facial features such as nose, mouth, eyes, etc.<br>

</p>
<hr>

</div>

<h5 id="may10">27 May</h5>
<div class="level5">

<p>
 - Searching for libraries which give us the facial feature 'points', since they are extremely important for our project.<br>

 - <code><strong>Interesting Libraries : </strong></code><br>

 - <strong>FLANDMARK</strong>, it finds 7 feature points : <a href="http://cmp.felk.cvut.cz/~uricamic/flandmark/" class="urlextern" title="http://cmp.felk.cvut.cz/~uricamic/flandmark/" rel="nofollow">http://cmp.felk.cvut.cz/~uricamic/flandmark/</a>. (For C++)<br>

 - <strong>ASMLIB</strong>, it finds many more feature points : <a href="https://code.google.com/p/asmlib-opencv/" class="urlextern" title="https://code.google.com/p/asmlib-opencv/" rel="nofollow">https://code.google.com/p/asmlib-opencv/</a>. (For C++)<br>

 - <strong>STASM</strong>, it's surely a keeper, gives 77 feature points and the results are really good : <a href="http://www.milbo.users.sonic.net/stasm/" class="urlextern" title="http://www.milbo.users.sonic.net/stasm/" rel="nofollow">http://www.milbo.users.sonic.net/stasm/</a>. (For C++)<br>

 - <strong>PYINYOURFACE</strong>, Looks promising : <a href="https://code.google.com/p/pyinyourface/" class="urlextern" title="https://code.google.com/p/pyinyourface/" rel="nofollow">https://code.google.com/p/pyinyourface/</a>. (For Python)<br>

</p>
<hr>

</div>

<h5 id="may11">28 May</h5>
<div class="level5">

<p>
 - Thought about the various models we could implement, the best options were :<br>

 - Classifying on the basis of motion of the face: The model is based on <a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:en.wikipedia.org_wiki_facial_action_coding_system" class="wikilink2" title="2014pc:en.wikipedia.org_wiki_facial_action_coding_system" rel="nofollow">FACS (Facial Action Coding System)</a>. This can only classify emotions in a video.<br>

 - Classify on the basis of still images: This model classifies only on the basis of the facial expression at the instant, this can classify emotions in both a video and an image.<br>

 - But sadly, both of these need libraries which are only available in C++. So we may need to install OpenCV for C++, and start from there.<br>

 - Meanwhile, we thought of other possible implementations of the emotion classifier, which can be done in python. The ideas were related to the still image method and are as follows :<br>

 <code><strong>(A)</strong></code> Convert the image to <a href="http://en.wikipedia.org/wiki/Eigenface#Eigenface_generation" class="urlextern" title="http://en.wikipedia.org/wiki/Eigenface#Eigenface_generation" rel="nofollow">Eigenfaces</a> and then compare them. <br>

 <code><strong>(B)</strong></code> Resize the image and construct a feature vector taking all the pixels as features and use SVM, kNN or other methods to classify. <br>

 <code><strong>(C)</strong></code> Convert the images to a feature vector, and then reduce the feature size by Principal Component Analysis. Then use classification methods on it. <br>

 <code><strong>(D)</strong></code> Instead of considering the whole face as features, use Haar Cascade detectors to find the nose, eyes and mouth, consider only these as features. Then use classification methods on it. <br>

 - The dataset we are planning to use is The Japanese Female Facial Expression (JAFFE) Database. It contains 213 images of 7 facial expressions including neutral posed by 10 Japanese female models.<br>

</p>
<hr>

</div>

<h5 id="may12">29 May</h5>
<div class="level5">

<p>
 - Started implementing method B first, since it's the most straight-forward.<br>

 - We divided the data into two parts, one the training part <code><strong>(Data X)</strong></code>, and the other the testing part <code><strong>(Data Y)</strong></code>.<br>
 <br>

 - <code><strong>Implementation details :</strong></code><br>
<br>

 - Did some general housekeeping such as:
</p>
<ol>
<li class="level1"><div class="li"> Using Haar Cascade detector to find the face in the images.<br>
</div>
</li>
<li class="level2"><div class="li"> Then crop the image to take only the face region as a new image.<br>
</div>
</li>
<li class="level2"><div class="li"> We then convert it to grayscale and resize the image to (25,25), thus giving a feature vector of dimension 625.<br>
</div>
</li>
</ol>

<p>
 - Now that are feature vector is ready, we train and check the result of many classification methods. The classification algorithms used and their results are as follows:<br>

</p>
<ol>
<li class="level1"><div class="li"> <code><strong>Support Vector Machines:</strong></code> This wasn't expected to work very well since our training data was very small. But it finally gave an recognition accuracy of about 0.35 on 'Data Y'.<br>
</div>
</li>
<li class="level2"><div class="li"> <code><strong>Support Vector Machines (With weights):</strong></code> We suspected that some specific emotions were causing havoc, so we tried playing with the weights. But it only improved marginally i.e. recognition accuracy was 0.4.<br>
</div>
</li>
<li class="level2"><div class="li"> <code><strong>Decision trees:</strong></code> They scale well even if we have a small dataset. So we had some expectations with it. The single decision gave a recognition accuracy of about 0.3.<br>
</div>
</li>
<li class="level2"><div class="li"> <code><strong>Randomised Decision trees:</strong></code> This one is a better algorithm for our case, so it should have worked. It consists of voting of many weak decision trees. This didn't disappoint and gave a recognition accuracy of about 0.5 …. Still bad, I know :(<br>
</div>
</li>
</ol>

<p>
 - Then we tried other variations like one vs one classifiers, one vs the rest classifiers, but the result was almost the same.<br>

 - Hmmm… well, if you gave a monkey some placards with different emotions written on it, and asked him to tell your emotion. It would do a better job than our classifier. Despite giving an accuracy of about 0.4 (on average). All the classifiers faltered big time (it was similar to random guesses) when they dealt with real data (through my webcam). Now the possible reasons are… my webcam is bad (I don't want that) or our classifier sucks (I don't want that either!).<br>

 - On some debugging, it was found that the classifier always gave almost a constant answer for real data. This is a huge mystery. <code><strong>(Solved, see 31 May)</strong></code><br>

</p>
<hr>

</div>

<h5 id="may13">30 May</h5>
<div class="level5">

<p>
 - So a new day starts, hopefully better than yesterday.<br>

 - The day was spent mainly on reading some (i.e. many) research papers. Some of the interesting ones are as follows:<br>

 - Using PCA algorithm: <a href="http://www.ijsce.org/attachments/File/v3i4/D1824093413.pdf" class="urlextern" title="http://www.ijsce.org/attachments/File/v3i4/D1824093413.pdf" rel="nofollow">http://www.ijsce.org/attachments/File/v3i4/D1824093413.pdf</a><br>

 - Using AAM models: <a href="http://people.uncw.edu/pattersone/research/publications/RatliffPatterson_HCI2008.pdf" class="urlextern" title="http://people.uncw.edu/pattersone/research/publications/RatliffPatterson_HCI2008.pdf" rel="nofollow">http://people.uncw.edu/pattersone/research/publications/RatliffPatterson_HCI2008.pdf</a><br>

 - Using PCA method and then kNN: <a href="http://www.ijcaonline.org/volume9/number12/pxc3871933.pdf" class="urlextern" title="http://www.ijcaonline.org/volume9/number12/pxc3871933.pdf" rel="nofollow">http://www.ijcaonline.org/volume9/number12/pxc3871933.pdf</a><br>

 - Uses motion of the face and then SVM: <a href="http://www.cs.cmu.edu/~pmichel/publications/Michel-FacExpRecSVMAbstract.pdf" class="urlextern" title="http://www.cs.cmu.edu/~pmichel/publications/Michel-FacExpRecSVMAbstract.pdf" rel="nofollow">http://www.cs.cmu.edu/~pmichel/publications/Michel-FacExpRecSVMAbstract.pdf</a><br>

 - Uses PCA and neural networks: <a href="http://uav.ro/stiinte_exacte/journal/index.php/TAMCS/article/viewFile/2/11" class="urlextern" title="http://uav.ro/stiinte_exacte/journal/index.php/TAMCS/article/viewFile/2/11" rel="nofollow">http://uav.ro/stiinte_exacte/journal/index.php/TAMCS/article/viewFile/2/11</a><br>

</p>
<hr>

</div>

<h5 id="may14">31 May</h5>
<div class="level5">

<p>
 - Finally found out the explanation to the mystery, our code wasn't robust. It needed exact lighting conditions (same as that of the training set), same contrast and other things. <code><strong>(Solution to Mystery)</strong></code><br>

 - Started implementing method C. Many research papers say it gives good results.<br>

 - We divided the data into two parts, one the training part <code><strong>(Data X)</strong></code>, and the other the testing part <code><strong>(Data Y)</strong></code>.<br>
 <br>

 - <code><strong>Implementation details :</strong></code><br>
<br>

 - Did some general housekeeping such as:
</p>
<ol>
<li class="level1"><div class="li"> Using Haar Cascade detector to find the face in the images.<br>
</div>
</li>
<li class="level2"><div class="li"> Then crop the image to take only the face region as a new image.<br>
</div>
</li>
<li class="level2"><div class="li"> We then convert it to grayscale and resize the image to (200,140), this large image size will be taken care of by the PCA part.</div>
</li>
</ol>

<p>
 - We read about implementation of PCA from  <a href="https://www.ce.yildiz.edu.tr/personal/songul/file/1097/principal_components.pdf" class="urlextern" title="https://www.ce.yildiz.edu.tr/personal/songul/file/1097/principal_components.pdf" rel="nofollow">A Good tutorial</a> and <a href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" class="urlextern" title="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" rel="nofollow">Implementing PCA in Python</a>.<br>

 - Finally decided to use a library function and not waste time, since we are already behind. Read, <a href="http://stackoverflow.com/questions/1730600/principal-component-analysis-in-python" class="urlextern" title="http://stackoverflow.com/questions/1730600/principal-component-analysis-in-python" rel="nofollow">Answers on Stack overflow</a>, <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" class="urlextern" title="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="nofollow">A sklearn function for PCA</a> and <a href="https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml" class="urlextern" title="https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml" rel="nofollow">A matplotlib function</a>.<br>

 - Decided to use the sklearn function. Extracted the features from the images and then finally made a reduced feature vector using the PCA function.<br>

 - Trained it using a self implemented algorithm similar to the nearest k neighbors. <br>

 - The result was again disheartening, a recognition accuracy of about 0.35. But we felt, it will be better than the other classifiers on real data . Tried it on my webcam, and the results were poor (again, oh boy!).<br>

 - But it was expected, since we didn't adjust the lighting conditions.<br>

 - Now working on improving the robustness of the program.<br>

</p>
<hr>

</div>

<h5 id="june">1 June</h5>
<div class="level5">

<p>
 - Read about normalizing image intensity: <a href="http://stackoverflow.com/questions/7116113/normalize-histogram-brightness-and-contrast-of-a-set-of-images-using-python-im" class="urlextern" title="http://stackoverflow.com/questions/7116113/normalize-histogram-brightness-and-contrast-of-a-set-of-images-using-python-im" rel="nofollow">Stack overflow</a>, <a href="http://en.wikipedia.org/wiki/Normalization_%28image_processing%29" class="urlextern" title="http://en.wikipedia.org/wiki/Normalization_%28image_processing%29" rel="nofollow">Wikipedia</a>, <a href="http://www.janeriksolem.net/2009/06/histogram-equalization-with-python-and.html" class="urlextern" title="http://www.janeriksolem.net/2009/06/histogram-equalization-with-python-and.html" rel="nofollow">It's implementation in Python</a> and <a href="http://effbot.org/zone/pil-histogram-equalization.htm" class="urlextern" title="http://effbot.org/zone/pil-histogram-equalization.htm" rel="nofollow">some others</a>.<br>

 - Tried to use binary images as features, it gave poor results compared to the grayscale one.<br>

</p>
<hr>

</div>

<h5 id="june1">2 June</h5>
<div class="level5">

<p>
 - Tried to apply a mixture of ideas for the classifier i.e. used idea (D) and tried it with (C) and (B).<br>

 - Tried Haar Cascade classifier for finding facial features, the results were a bit poor i.e. it detected 2 noses, 4 eyes and a couple of mouths in a single face!.<br>

 - Played around with the threshold for the respective functions to increase accuracy.<br>

 - Much to our delight, it rocked! The accuracy for the nose detector was 100% (it always found one nose). The eye pair detector found 2 eyes almost 96% of the time. The mouth detector worked correctly for 92% of the cases.<br>

 - Used the Haar Cascade detectors and clubbed it with idea (B).<br>

 - <code><strong>(D) with (B)</strong></code>: It gave a recognition accuracy of about 0.3 with completely different faces, and 0.55 otherwise.<br>

</p>
<hr>

</div>

<h5 id="june2">3 June</h5>
<div class="level5">

<p>
 - Implemented the algorithmwith (D) and (C).<br>

 - <code><strong>(D) with (C)</strong></code>: It gave a recognition accuracy of about 0.35 with completely different faces, and 0.65 otherwise.<br>

</p>
<hr>

</div>

<h5 id="june3">4 June</h5>
<div class="level5">

<p>
 - Thinking about the possible improvements in the feature extraction process.<br>

 - The area with the largest scope for improvement is robustness. The things we have shortlisted are as follows:
</p>
<ol>
<li class="level1"><div class="li"> Noise reduction in original image<br>
</div>
</li>
<li class="level1"><div class="li"> Image Intensity normalization<br>
</div>
</li>
</ol>

<p>
 - So decide to do some research on them.<br>

</p>
<hr>

</div>

<h5 id="june4">5 June</h5>
<div class="level5">

<p>
 - For noise reduction, we read about different methods (in image smoothing) : Bilateral filter, Gaussian blur, Image noise, Median filter and Noise reduction.<br>

 - Finally we searched for functions present in opencv for image smoothing. We found a program performing <a href="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.html" class="urlextern" title="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.html" rel="nofollow">normalized box filtering</a>.<br>

 - We decided to blur the image using a 2×2 kernel and using the opencv <a href="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.html" class="urlextern" title="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/filter_2d/filter_2d.html" rel="nofollow">filter2D</a> function.<br>

 - Tried this method on our images, the results were nice. The images looked much more smooth, and unnecessary  blots were also reduced.<br>

</p>
<hr>

</div>

<h5 id="june5">6 June</h5>
<div class="level5">

<p>
 - For image intensity normalisation, we read different sources : <a href="http://www.mathworks.in/matlabcentral/answers/10716-how-to-normalise-image-intensity" class="urlextern" title="http://www.mathworks.in/matlabcentral/answers/10716-how-to-normalise-image-intensity" rel="nofollow">Source (1)</a>, <a href="http://en.wikipedia.org/wiki/Color_normalization" class="urlextern" title="http://en.wikipedia.org/wiki/Color_normalization" rel="nofollow">Source (2)</a>, <a href="http://en.wikipedia.org/wiki/Normalization_(image_processing)" class="urlextern" title="http://en.wikipedia.org/wiki/Normalization_(image_processing)" rel="nofollow">Source (3)</a>, <a href="http://docs.opencv.org/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.html" class="urlextern" title="http://docs.opencv.org/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.html" rel="nofollow">Source (4)</a>, <a href="http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html" class="urlextern" title="http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html" rel="nofollow">Source (5)</a>.<br>

 - Finally decide to use Histogram Normalisation (Source 4) and CLAHE algorithm (Source 5).<br>

 - Used the equaliseHist() function in opencv and tested it on our images, it worked very nicely.<br>

 - Integrated it into our code along with the noise reducer.<br>

</p>
<hr>

</div>

<h5 id="june6">7 June</h5>
<div class="level5">

<p>
 - After all those modifications, we tested our code. And to our delight the recognition accuracy for different faces went up to 0.5, and 0.72 otherwise.<br>

 - The accuracy was dragged down by some emotions such as disgust and fear.<br>

 - We tried CLAHE algorithm, but it reduced the overall accuracy.<br>

</p>
<hr>

</div>

<h5 id="june7">8-9 June</h5>
<div class="level5">

<p>
 - A relaxing break in Nainital.<br>

</p>
<hr>

</div>

<h5 id="june8">10 June</h5>
<div class="level5">

<p>
 - Tried and tested other approaches, like changing class weights in LinearSVC, using decision trees, etc.<br>

The accuracy remained roughly same.<br>

 - Tried a slightly different approach for normalisation inspired by CLAHE. Instead of normalising the entire image histogram, we could extract only the face from the image and normalise it. This helps in case of a disturbing background.<br>

 - The recognition accuracy became 0.45 after this, but we decided to keep it since it felt alright.<br>

</p>
<hr>

</div>

<h5 id="june9">11 June</h5>
<div class="level5">

<p>
 - Tried classifying our own emotions from the webcam.<br>

 - The results were not bad, but not good either. So we decided that a database with only Japanese women as subjects isn't really a good idea. So we searched for a different database, and found many others : <a href="http://pics.psych.stir.ac.uk/2D_face_sets.htm" class="urlextern" title="http://pics.psych.stir.ac.uk/2D_face_sets.htm" rel="nofollow">Link for many Databases</a>.<br>

</p>
<hr>

</div>

<h5 id="june10">12 June</h5>
<div class="level5">

<p>
 - Downloaded an database with around 500 images, of which 300 were usable.<br>

 - The database consisted of many people with different ages and genders. So we were hopeful it would help our cause.<br>

 - Trained the classifier with it. The final recognition accuracy was 0.43 for different faces, and 0.62 otherwise.<br>

 - For real world data it worked considerably better than the previous one. We didn't formally calculate the recognition accuracy, but it was good for emotions like happiness, surprise, anger, disgust and poor otherwise.<br>

</p>
<hr>

</div>

<h5 id="june11">13 June</h5>
<div class="level5">

<p>
 - Started working on the audio classifier.<br>

 - Read from some sources about emotion recognition using speech:
</p>
<ol>
<li class="level1"><div class="li"> <a href="http://www.sciencedirect.com/science/article/pii/S0031320310004619" class="urlextern" title="http://www.sciencedirect.com/science/article/pii/S0031320310004619" rel="nofollow">Survey on speech emotion recognition: Features, classification schemes, and databases</a></div>
</li>
<li class="level1"><div class="li"> <a href="http://www.sciencedirect.com/science/article/pii/S0167639303000992" class="urlextern" title="http://www.sciencedirect.com/science/article/pii/S0167639303000992" rel="nofollow">Speech emotion recognition using hidden Markov models</a></div>
</li>
<li class="level1"><div class="li"> <a href="http://www.informatik.uni-augsburg.de/lehrstuehle/hcm/projects/tools/emovoice/" class="urlextern" title="http://www.informatik.uni-augsburg.de/lehrstuehle/hcm/projects/tools/emovoice/" rel="nofollow">EmoVoice</a></div>
</li>
<li class="level1"><div class="li"> <a href="http://www.sersc.org/journals/IJSH/vol6_no2_2012/15.pdf" class="urlextern" title="http://www.sersc.org/journals/IJSH/vol6_no2_2012/15.pdf" rel="nofollow">Speech Emotion Recognition Using SVM</a></div>
</li>
</ol>

<p>
 - Started looking for suitable libraries which would help in extracting the audio features.<br>

</p>
<hr>

</div>

<h5 id="june12">14 June</h5>
<div class="level5">

<p>
 - Tried installing <a href="http://opensmile.sourceforge.net/" class="urlextern" title="http://opensmile.sourceforge.net/" rel="nofollow">OPENSMILE</a>, its a library which works in C++. 
 - Successful in installing it after a few failed attempts.<br>

 - Read the docs and finally concluded that it won't do us any good. But still it has nice features if you ever want to do some audio processing.<br>

 - So we again moved back to python, read about libraries like pyaudio, pythonspeechfeatures, pythoninmusic and others.<br>

 - Tried installing some of them, but they were so unpopular that we couldn't use setuptools (*SIGH*).<br>

 - A way around this (jugaad) was to go to the source code and directly use all the functions from there, copying them into our code.<br>

</p>
<hr>

</div>

<h5 id="june13">15 June</h5>
<div class="level5">

<p>
 - So we tried the jugaad, and it worked (*YAY*). Then we tried some basic i/o and stuff which the library provided.<br>

 - Then we read about the possible features of speech we could use:<br>

</p>
<ol>
<li class="level1"><div class="li"> Pitch features </div>
</li>
<li class="level1"><div class="li"> Energy contours</div>
</li>
<li class="level1"><div class="li"> MFCC and related stuff (it's median, mean, maxima, etc)</div>
</li>
<li class="level1"><div class="li"> LFPC and related data</div>
</li>
</ol>

<p>
 - Due to the lack of appropriate libraries, we weren't able to extract anything except the MFCC. And the shortage of time prevented us from implementing the other stuff by ourselves.<br>

 - So we decided to make a basic model with only mfcc features.<br>

</p>
<hr>

</div>

<h5 id="june14">16 June</h5>
<div class="level5">

<p>
 - The database we used was “Berlin emotional speech database”.<br>

 - It consisted of 6 emotions instead of the 7 we were aiming for. But we decided to add our own data for the the 7th one later.<br>

 - We divided the data into two parts, one the training part <code><strong>(Data X)</strong></code>, and the other the testing part <code><strong>(Data Y)</strong></code>.<br>
 <br>

 - <code><strong>Implementation details :</strong></code><br>
<br>

 - Used pyaudio and scikits.audiolab for audio input-output.<br>

 - We calculated MFCC features for all the frames of an audio clip. For each frame, the MFCC was a 13 dimensional vector.<br>

 - So there was a problem, we couldn't make all of them features, since the dimension of the feature vector would be overwhelming. So we read about vector quantization.<br>

 - We used the scipy.cluster library for vector quantization.<br>

 - So we had codes with values from 0-63, then each set of mfcc vector (13 values) was assigned a code using the scipy.cluster algorithm.<br>

 - Thus we had a sequence of numbers ranging from 0-63. This step brought down the initial problem to a problem which is well suited for Markov Models.<br>

 - We read about Hidden Markov Models and Gaussian mixture models.<br>

 - We decided to use HMM as the classifier, but finding a good library for it was again a problem.<br>

</p>
<hr>

</div>

<h5 id="june15">17 June</h5>
<div class="level5">

<p>
 - We tried the sklearn hmm, but it wasn't maintained for along time and gave us a lot of problems.<br>

 - Then surfed the internet for other sources, found some homebrew codes but they didn't satisfy our needs.<br>

 - Found one implementation, but when we used it, it gave a lot of errors. We then tried to debug the whole source code (*IN VAIN*),<br>

</p>
<hr>

</div>

<h5 id="june16">18 June</h5>
<div class="level5">

<p>
 - So hmm's were the ones designed for the job, but due to the lack of libraries we couldn't use them.<br>

 - We then tried our good ol' SVM and nearest K algorithms.<br>

 - SVM gave a recognition rate of 0.25, while nearest K was at 0.32 (*DON'T LAUGH*).<br>

 - Then we tried removing the whole vector quantization thingy. Instead we directly used all the 13 mfcc values.<br>

 - It helped a bit, the recognition rate was up to 0.43 for SVM, and 0.45 for nearest K.<br>

</p>
<hr>

</div>

<h5 id="june17">19 June</h5>
<div class="level5">

<p>
 - We then tried modifying the number of frames we considered at a time, to see if it changed the recognition rate. The maxima was attained for 100-200 frames.<br>

 - For 100 frames, the recognition rate was 0.525 for SVM, and 0.5 for nearest K.<br>

 - And just for those who mock us. The recognition rate for humans on the same database is about 0.7. So our classifier isn't extremely bad.<br>

 - We tried to experiment with other possible features like mean, variance and extremes.<br>

 - The approach really helped, the dimension was now reduced to 17 from the previous 1300, and the accuracy was almost the same.<br>

</p>
<hr>

</div>

<h5 id="june18">20 June</h5>
<div class="level5">

<p>
 - Next we thought of sorting the mfcc vectors according to some specific element (maybe the second element in the vector). The inspiration came from the fact that the sounds may be displaced a bit.<br>

 - So we ran a loop to see which index is the best. And the first and third ones were the best. The recognition rate for them were around 0.58, sometimes 0.64.<br>

</p>
<hr>

</div>

<h5 id="june19">21 June</h5>
<div class="level5">

<p>
 - We tested the classifier on microphone input, and it messed up really really bad!<br>

 - The output was invariably sad. Again the same problem like the image classifier, the classifier wasn't robust.<br>

 - So we read about the things we could do, noise reduction, loudness equalization.<br>

 - To use it we needed another command line tool called <a href="http://sox.sourceforge.net/sox.html" class="urlextern" title="http://sox.sourceforge.net/sox.html" rel="nofollow">SoX</a>. Fortunately, it installed without any major troubles.<br>

 - We wrote a code for nose reduction, which would hopefully make the classifier work well in tough situations.<br>

</p>
<hr>

</div>

<h5 id="june20">22 June</h5>
<div class="level5">

<p>
 - We started integrating pickle library in our codes, so that the classifiers could be loaded quickly.<br>

 - Finished making pickle files for all the classifiers.<br>

 - Selected the best classifiers, i.e. ones with the highest accuracy.<br>

 - Merged all of them together into a single program with a basic python interface.<br>

 - Implemented classifier merging, the whole aim of the project and finished up the main program.<br>

</p>
<hr>

<p>
Finally after a lot of hardships our project is completed :)
</p>

</div>

                    <!-- wikipage stop -->
                                    </div>

                <div class="docInfo"><bdi>2014pc/view_projects06.txt</bdi> · Last modified: 2014/06/24 15:49 by <bdi>pcp06</bdi></div>

                            </div></div><!-- /content -->

            <hr class="a11y">

            <!-- PAGE ACTIONS -->
            <div id="dokuwiki__pagetools">
                <h3 class="a11y">Page Tools</h3>
                <div class="tools">
                    <ul>
                        <li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=edit&rev=0" class="action source" accesskey="v" rel="nofollow" title="Show pagesource [V]"><span>Show pagesource</span></a></li><li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=revisions" class="action revs" accesskey="o" rel="nofollow" title="Old revisions [O]"><span>Old revisions</span></a></li><li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06&do=backlink" class="action backlink" rel="nofollow" title="Backlinks"><span>Backlinks</span></a></li><li><a href="http://students.iitk.ac.in/projects/wiki/doku.php?id=2014pc:view_projects06#dokuwiki__top" class="action top" accesskey="t" rel="nofollow" title="Back to top [T]"><span>Back to top</span></a></li>                    </ul>
                </div>
            </div>
        </div><!-- /wrapper -->

        
<!-- ********** FOOTER ********** -->
<div id="dokuwiki__footer"><div class="pad">
    <div class="license">Except where otherwise noted, content on this wiki is licensed under the following license: <bdi><a href="http://creativecommons.org/licenses/by-sa/3.0/" rel="license" class="urlextern">CC Attribution-Share Alike 3.0 Unported</a></bdi></div>
    <div class="buttons">
        <a href="http://creativecommons.org/licenses/by-sa/3.0/" rel="license"><img src="./emodet_wiki_files/cc-by-sa.png" alt="CC Attribution-Share Alike 3.0 Unported"></a>        <a href="http://www.dokuwiki.org/donate" title="Donate"><img src="./emodet_wiki_files/button-donate.gif" width="80" height="15" alt="Donate"></a>
        <a href="http://www.php.net/" title="Powered by PHP"><img src="./emodet_wiki_files/button-php.gif" width="80" height="15" alt="Powered by PHP"></a>
        <a href="http://validator.w3.org/check/referer" title="Valid HTML5"><img src="./emodet_wiki_files/button-html5.png" width="80" height="15" alt="Valid HTML5"></a>
        <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS"><img src="./emodet_wiki_files/button-css.png" width="80" height="15" alt="Valid CSS"></a>
        <a href="http://dokuwiki.org/" title="Driven by DokuWiki"><img src="./emodet_wiki_files/button-dw.png" width="80" height="15" alt="Driven by DokuWiki"></a>
    </div>
</div></div><!-- /footer -->

    </div></div><!-- /site -->

    <div class="no"><img src="./emodet_wiki_files/indexer.php" width="2" height="1" alt=""></div>
    <div id="screen__mode" class="no"></div>    <!--[if ( lte IE 7 | IE 8 ) ]></div><![endif]-->


</body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>